{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основы MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В своей сути MapRedcue это очень простая парадигма. Допустим у нас есть датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:25:44.158742Z",
     "start_time": "2021-01-25T20:25:41.796781Z"
    }
   },
   "outputs": [],
   "source": [
    "! curl https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv > tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 tweets_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим в этом датасете что-нибудь найти. Например (сюрприз-сюрприз), посчитать количество уникальных слов. Мы могли бы сделать что-то такое:\n",
    "\n",
    "#### Вариант 1 \n",
    "Используем исключительно питон и наивный алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:30:27.659125Z",
     "start_time": "2021-01-25T20:30:25.819459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t268703\n",
      "co\t250375\n",
      "https\t221366\n",
      "the\t69350\n",
      "to\t55972\n",
      "a\t43420\n",
      "in\t37099\n",
      "s\t36085\n",
      "of\t33579\n",
      "http\t28661\n",
      "CPU times: user 4.42 s, sys: 59.8 ms, total: 4.48 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "\n",
    "counter = Counter()\n",
    "pattern = re.compile(r\"[a-z]+\")\n",
    "\n",
    "with open('tweets_1.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            counter[word] += 1\n",
    "\n",
    "for word, count in counter.most_common(10):\n",
    "    print(f\"{word}\\t{count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:30:53.751130Z",
     "start_time": "2021-01-25T20:30:53.746887Z"
    }
   },
   "source": [
    "Такое сработает только если у нас не очень много данных и они все вмещаются в оперативную память"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91M\ttweets_1.csv\r\n"
     ]
    }
   ],
   "source": [
    "! du -h tweets_1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вариант 2\n",
    "Используем парадигму Map Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере у нас всего 90 мегабайт данных, и на моем компьютере они обрабатываются за примерно 30 секунд с помощью питона. Теперь представим (это достаточно несложно), что у нас приходит новых данных приходит _десятки терабайт_ в сутки. Такое уже не поместится ни в один сервер, поэтому нам нужно придумать что-нибудь похитрее.\n",
    "\n",
    "MapReduce как раз является парадигмой, помогающей обрабатывать большие объемы данных, за счет простоты своего устройства.\n",
    "\n",
    "Приятная новость - для того, чтобы понять и научиться программировать программы в парадигме MapReduce вам потребуется... **5 секунд!**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/ADKosm/lsml-2021-public/main/imgs/you-know-mapreduce.png\" width=\"400\">\n",
    "\n",
    "Все потому что вы уже прошли семинар по Bash и научились составлять большие программы в виде компоновки небольших  программ, соединенных пайпами. По своей сути программа на MapReduce - это хорошо отмасштабированная программа вида\n",
    "\n",
    "```bash\n",
    "cat data.txt | map | sort | reduce\n",
    "```\n",
    "\n",
    "Сортировку за вас выполняет сам фреймворк (и ее вы можете дополнительно настроить точно такое как и команду sort). А также он сам разбивает данные на части и параллельно запускает операции map и reduce. \n",
    "\n",
    "Таким образом на самом деле Hadoop - это всего лишь гигантская машина сортировки, которая дополнительно дает вам некоторые гарантии:\n",
    "\n",
    "* Для всех данных параллельно будет применена операция map\n",
    "* Данные будут отсортированы по указанному вами ключу\n",
    "* Каждый ключ будет целиком передан на один и только один reduce\n",
    "\n",
    "Программисту остается реализовать программу, которая состоит из двух компонент: `map` и `reduce`. \n",
    "\n",
    "Операция `map` -- это просто функция из одного элемента в другой элемент, у которого есть первичный ключ. \n",
    "\n",
    "Операция `reduce` -- это коммутативная и ассоциативная агрегация всех элементов по ключу. Чтобы эти операции совершить, надо разбить весь вход на куски данных и отправить их на машины, чтобы они выполнялись в параллель, а весь выход операции map идёт в операцию shuffle, которая по одним и тем же ключам определяет записи на одинаковые хосты. \n",
    "\n",
    "В итоге получается, что мы можем спокойно увеличивать количество worker'ов для map операций и с увеличением количества данных мы лишь будем линейно утилизировать количество машин, то же самое с операцией reduce -- мы можем добавлять машины с ростом увеличения количества ключей линейно, не боясь того, что мы не можем позволить на одной какой-то машине больше памяти или диска.\n",
    "\n",
    "Давайте напишем маппер и редьюсер на питоне для этой задачи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\r\n"
     ]
    }
   ],
   "source": [
    "! sudo pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:45:38.075819Z",
     "start_time": "2021-01-25T20:45:38.069953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount.py\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv.reader(iter(sys.stdin.readline, '')):\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            print(\"{}\\t{}\".format(word, 1))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    word, number = next(sys.stdin).split('\\t')\n",
    "    number = int(number)\n",
    "    for line in sys.stdin:\n",
    "        current_word, current_number = line.split('\\t')\n",
    "        current_number = int(current_number)\n",
    "        if current_word != word:\n",
    "            print(\"{}\\t{}\".format(word, number))\n",
    "            word = current_word\n",
    "            number = current_number\n",
    "        else:\n",
    "            number += current_number\n",
    "    print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно еще удалить голову у таблицы, иначе подсчеты могут быть некоректными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -i -e '1'd tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 2 tweets_1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "a\t1\r\n",
      "sort: write failed: 'standard output': Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "! cat tweets_1.csv | python wordcount.py map | sort -k1,1 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:47:23.728893Z",
     "start_time": "2021-01-25T20:46:51.895208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 243891/243891 [00:09<00:00, 25475.62it/s]\n",
      "CPU times: user 253 ms, sys: 23.5 ms, total: 276 ms\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! cat tweets_1.csv | \\\n",
    "    tqdm --total $(cat tweets_1.csv | wc -l)| \\\n",
    "    python wordcount.py map | \\\n",
    "    sort -k1,1 | \\\n",
    "    python wordcount.py reduce > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t43420\r\n",
      "aa\t151\r\n",
      "aaa\t13\r\n",
      "aaaaaa\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaaall\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaaand\t2\r\n",
      "aaaaaaargh\t1\r\n",
      "aaaaaand\t2\r\n"
     ]
    }
   ],
   "source": [
    "! head result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:50:28.963822Z",
     "start_time": "2021-01-25T20:50:28.265568Z"
    }
   },
   "source": [
    "Отлично! Слова есть, осталось только найти top-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10.py\n",
    "import sys\n",
    "\n",
    "\n",
    "def _rewind_stream(stream):\n",
    "    for _ in stream:\n",
    "        pass\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    for row in sys.stdin:\n",
    "        key, value = row.split('\\t')\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for _ in range(10):\n",
    "        key, _ = next(sys.stdin).split('\\t')\n",
    "        word, count = key.split(\"+\")\n",
    "        print(\"{}\\t{}\".format(word, count))\n",
    "    _rewind_stream(sys.stdin)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 346613/346613 [00:00<00:00, 596413.03it/s]\n"
     ]
    }
   ],
   "source": [
    "! cat result.txt | \\\n",
    "    tqdm --total $(cat result.txt | wc -l) | \\\n",
    "    python top10.py map | \\\n",
    "    sort -t'+' -k2,2nr -k1,1 | \\\n",
    "    python top10.py reduce > top-10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t268703\r\n",
      "co\t250375\r\n",
      "https\t221366\r\n",
      "the\t69350\r\n",
      "to\t55972\r\n",
      "a\t43420\r\n",
      "in\t37099\r\n",
      "s\t36085\r\n",
      "of\t33579\r\n",
      "http\t28661\r\n"
     ]
    }
   ],
   "source": [
    "! cat top-10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На MapReduce мы задачу переписали, однако быстрее работать она пока не стала. Все дело в том, что мы это еще не на кластере запускали! Время запускать все на настоящем кластере!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем данные в HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T20:52:22.492958Z",
     "start_time": "2021-01-25T20:52:22.382757Z"
    }
   },
   "source": [
    "При работе с HDFS нужно понимать, что есть два места, где хранятся данные\n",
    "\n",
    "1. На локальных жестких дисках машин кластера - это деволтная система, на нее можно посмотреть через `hdfs dfs -ls /`\n",
    "\n",
    "2. В Object Storage - для работы с ней, нужно указывать путь до бакета - `hdfs dfs -ls s3a://lsml2022alexius/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv --> IRAhandle_tweets_1.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  17.2M      0  0:00:05  0:00:05 --:--:-- 22.4M\n",
      "\n",
      "[2/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv --> IRAhandle_tweets_2.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  20.7M      0  0:00:04  0:00:04 --:--:-- 27.0M\n",
      "\n",
      "[3/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv --> IRAhandle_tweets_3.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  21.0M      0  0:00:04  0:00:04 --:--:-- 21.0M\n",
      "\n",
      "[4/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv --> IRAhandle_tweets_4.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  21.9M      0  0:00:04  0:00:04 --:--:-- 21.9M\n",
      "\n",
      "[5/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv --> IRAhandle_tweets_5.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  22.2M      0  0:00:04  0:00:04 --:--:-- 22.2M\n",
      "\n",
      "[6/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv --> IRAhandle_tweets_6.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  18.5M      0  0:00:04  0:00:04 --:--:-- 18.5M\n",
      "\n",
      "[7/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv --> IRAhandle_tweets_7.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  22.2M      0  0:00:04  0:00:04 --:--:-- 22.2M\n",
      "\n",
      "[8/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv --> IRAhandle_tweets_8.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  22.8M      0  0:00:03  0:00:03 --:--:-- 22.8M\n",
      "\n",
      "[9/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv --> IRAhandle_tweets_9.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  23.4M      0  0:00:03  0:00:03 --:--:-- 23.4M\n",
      "\n",
      "[10/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv --> IRAhandle_tweets_10.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  21.3M      0  0:00:04  0:00:04 --:--:-- 21.3M\n",
      "\n",
      "[11/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv --> IRAhandle_tweets_11.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 89.9M  100 89.9M    0     0  22.3M      0  0:00:04  0:00:04 --:--:-- 22.3M\n",
      "\n",
      "[12/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv --> IRAhandle_tweets_12.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  22.4M      0  0:00:04  0:00:04 --:--:-- 22.4M\n",
      "\n",
      "[13/13]: https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv --> IRAhandle_tweets_13.csv\n",
      "--_curl_--https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8045k  100 8045k    0     0  16.2M      0 --:--:-- --:--:-- --:--:-- 16.2M\n"
     ]
    }
   ],
   "source": [
    "! curl -O https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_{`seq -s , 1 13`}.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подформатируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1\n",
      "Finish 2\n",
      "Finish 3\n",
      "Finish 4\n",
      "Finish 5\n",
      "Finish 6\n",
      "Finish 7\n",
      "Finish 8\n",
      "Finish 9\n",
      "Finish 10\n",
      "Finish 11\n",
      "Finish 12\n",
      "Finish 13\n"
     ]
    }
   ],
   "source": [
    "! for i in {1..13}; do sed IRAhandle_tweets_$i.csv -i -e '1'd && echo \"Finish $i\" ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим отдельную папку для этих данных в HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwx------   - mapred hadoop          0 2023-01-06 08:41 /hadoop\r\n",
      "drwxr-xr-x   - hdfs   hadoop          0 2023-01-21 15:13 /system\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2023-01-06 08:40 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2023-01-22 18:09 /user\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2023-01-06 08:41 /var\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/data\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/data\n",
    "! hdfs dfs -mkdir -p /user/tweets/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: все команды для hdfs смотреть здесь - https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html\n",
    "\n",
    "Заливаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -put IRAhandle_tweets_* /user/tweets/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-24 15:47:52,085 INFO balancer.Balancer: namenodes  = [hdfs://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8020]\n",
      "2023-01-24 15:47:52,090 INFO balancer.Balancer: parameters = Balancer.BalancerParameters [BalancingPolicy.Node, threshold = 10.0, max idle iteration = 5, #excluded nodes = 0, #included nodes = 0, #source nodes = 0, #blockpools = 0, run during upgrade = false]\n",
      "2023-01-24 15:47:52,090 INFO balancer.Balancer: included nodes = []\n",
      "2023-01-24 15:47:52,090 INFO balancer.Balancer: excluded nodes = []\n",
      "2023-01-24 15:47:52,090 INFO balancer.Balancer: source nodes = []\n",
      "Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved  NameNode\n",
      "2023-01-24 15:47:52,094 INFO balancer.NameNodeConnector: getBlocks calls for hdfs://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8020 will be rate-limited to 20 per second\n",
      "2023-01-24 15:47:53,166 INFO balancer.Balancer: dfs.namenode.get-blocks.max-qps = 20 (default=20)\n",
      "2023-01-24 15:47:53,166 INFO balancer.Balancer: dfs.balancer.movedWinWidth = 5400000 (default=5400000)\n",
      "2023-01-24 15:47:53,166 INFO balancer.Balancer: dfs.balancer.moverThreads = 1000 (default=1000)\n",
      "2023-01-24 15:47:53,166 INFO balancer.Balancer: dfs.balancer.dispatcherThreads = 200 (default=200)\n",
      "2023-01-24 15:47:53,167 INFO balancer.Balancer: dfs.balancer.getBlocks.size = 2147483648 (default=2147483648)\n",
      "2023-01-24 15:47:53,167 INFO balancer.Balancer: dfs.balancer.getBlocks.min-block-size = 10485760 (default=10485760)\n",
      "2023-01-24 15:47:53,167 INFO balancer.Balancer: dfs.datanode.balance.max.concurrent.moves = 50 (default=50)\n",
      "2023-01-24 15:47:53,167 INFO balancer.Balancer: dfs.datanode.balance.bandwidthPerSec = 10485760 (default=10485760)\n",
      "2023-01-24 15:47:53,172 INFO balancer.Balancer: dfs.balancer.max-size-to-move = 10737418240 (default=10737418240)\n",
      "2023-01-24 15:47:53,172 INFO balancer.Balancer: dfs.blocksize = 268435456 (default=134217728)\n",
      "2023-01-24 15:47:53,193 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.37:9866\n",
      "2023-01-24 15:47:53,193 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.27:9866\n",
      "2023-01-24 15:47:53,194 INFO net.NetworkTopology: Adding a new node: /default-rack/10.128.0.30:9866\n",
      "2023-01-24 15:47:53,196 INFO balancer.Balancer: 0 over-utilized: []\n",
      "2023-01-24 15:47:53,196 INFO balancer.Balancer: 0 underutilized: []\n",
      "The cluster is balanced. Exiting...\n",
      "Jan 24, 2023 3:47:53 PM           0                  0 B                 0 B                0 B  hdfs://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8020\n",
      "Jan 24, 2023 3:47:53 PM  Balancing took 1.357 seconds\n"
     ]
    }
   ],
   "source": [
    "! sudo mkdir -p /usr/lib/hadoop/logs\n",
    "! sudo chmod 0777 /usr/lib/hadoop/logs\n",
    "! sudo -u hdfs hdfs balancer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 1585661300736 (1.44 TB)\r\n",
      "Present Capacity: 1483057369088 (1.35 TB)\r\n",
      "DFS Remaining: 1419503308800 (1.29 TB)\r\n",
      "DFS Used: 63554060288 (59.19 GB)\r\n",
      "DFS Used%: 4.29%\r\n",
      "Replicated Blocks:\r\n",
      "\tUnder replicated blocks: 0\r\n",
      "\tBlocks with corrupt replicas: 0\r\n",
      "\tMissing blocks: 0\r\n",
      "\tMissing blocks (with replication factor 1): 0\r\n",
      "\tLow redundancy blocks with highest priority to recover: 0\r\n",
      "\tPending deletion blocks: 0\r\n",
      "Erasure Coded Block Groups: \r\n",
      "\tLow redundancy block groups: 0\r\n",
      "\tBlock groups with corrupt internal blocks: 0\r\n",
      "\tMissing block groups: 0\r\n",
      "\tLow redundancy blocks with highest priority to recover: 0\r\n",
      "\tPending deletion blocks: 0\r\n",
      "\r\n",
      "-------------------------------------------------\r\n",
      "Live datanodes (3):\r\n",
      "\r\n",
      "Name: 10.128.0.27:9866 (rc1a-dataproc-d-1jipfldazn1nw9u1.mdb.yandexcloud.net)\r\n",
      "Hostname: rc1a-dataproc-d-1jipfldazn1nw9u1.mdb.yandexcloud.net\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 528553766912 (492.25 GB)\r\n",
      "DFS Used: 28756549468 (26.78 GB)\r\n",
      "Non DFS Used: 12659589284 (11.79 GB)\r\n",
      "DFS Remaining: 465623265280 (433.65 GB)\r\n",
      "DFS Used%: 5.44%\r\n",
      "DFS Remaining%: 88.09%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 1\r\n",
      "Last contact: Tue Jan 24 15:48:22 UTC 2023\r\n",
      "Last Block Report: Tue Jan 24 15:01:10 UTC 2023\r\n",
      "Num of Blocks: 448\r\n",
      "\r\n",
      "\r\n",
      "Name: 10.128.0.30:9866 (rc1a-dataproc-d-ffxjmugb9fqgvn59.mdb.yandexcloud.net)\r\n",
      "Hostname: rc1a-dataproc-d-ffxjmugb9fqgvn59.mdb.yandexcloud.net\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 528553766912 (492.25 GB)\r\n",
      "DFS Used: 17109728134 (15.93 GB)\r\n",
      "Non DFS Used: 12741960826 (11.87 GB)\r\n",
      "DFS Remaining: 477187801088 (444.42 GB)\r\n",
      "DFS Used%: 3.24%\r\n",
      "DFS Remaining%: 90.28%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 1\r\n",
      "Last contact: Tue Jan 24 15:48:21 UTC 2023\r\n",
      "Last Block Report: Tue Jan 24 15:01:13 UTC 2023\r\n",
      "Num of Blocks: 390\r\n",
      "\r\n",
      "\r\n",
      "Name: 10.128.0.37:9866 (rc1a-dataproc-d-z4c3crwrio0motqz.mdb.yandexcloud.net)\r\n",
      "Hostname: rc1a-dataproc-d-z4c3crwrio0motqz.mdb.yandexcloud.net\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 528553766912 (492.25 GB)\r\n",
      "DFS Used: 17687782686 (16.47 GB)\r\n",
      "Non DFS Used: 12659378914 (11.79 GB)\r\n",
      "DFS Remaining: 476692242432 (443.95 GB)\r\n",
      "DFS Used%: 3.35%\r\n",
      "DFS Remaining%: 90.19%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 1\r\n",
      "Last contact: Tue Jan 24 15:48:22 UTC 2023\r\n",
      "Last Block Report: Tue Jan 24 15:01:10 UTC 2023\r\n",
      "Num of Blocks: 126\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! sudo -u hdfs hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -u hdfs hdfs balancer -threshold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371561 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_1.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371615 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_10.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371552 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_11.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371703 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_12.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop    8238864 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_13.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371748 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_2.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371796 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_3.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371606 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_4.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371616 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_5.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371646 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_6.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371711 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_7.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371727 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_8.csv\r\n",
      "-rw-r--r--   1 ubuntu hadoop   94371542 2023-01-24 15:46 /user/tweets/data/IRAhandle_tweets_9.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запускаем MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, что скрипты на головной машине"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "import csv\r\n",
      "import re\r\n",
      "\r\n",
      "\r\n",
      "def mapper():\r\n",
      "    pattern = re.compile(r\"[a-z]+\")\r\n",
      "    for row in csv.reader(iter(sys.stdin.readline, '')):\r\n",
      "        content = row[2]\r\n",
      "        for match in pattern.finditer(content.lower()):\r\n",
      "            word = match.group(0)\r\n",
      "            print(\"{}\\t{}\".format(word, 1))\r\n",
      "\r\n",
      "\r\n",
      "def reducer():\r\n",
      "    word, number = next(sys.stdin).split('\\t')\r\n",
      "    number = int(number)\r\n",
      "    for line in sys.stdin:\r\n",
      "        current_word, current_number = line.split('\\t')\r\n",
      "        current_number = int(current_number)\r\n",
      "        if current_word != word:\r\n",
      "            print(\"{}\\t{}\".format(word, number))\r\n",
      "            word = current_word\r\n",
      "            number = current_number\r\n",
      "        else:\r\n",
      "            number += current_number\r\n",
      "    print(\"{}\\t{}\".format(word, number))\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    mr_command = sys.argv[1]\r\n",
      "    {\r\n",
      "        'map': mapper,\r\n",
      "        'reduce': reducer\r\n",
      "    }[mr_command]()\r\n"
     ]
    }
   ],
   "source": [
    "! cat wordcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\r\n",
      "\r\n",
      "\r\n",
      "def _rewind_stream(stream):\r\n",
      "    for _ in stream:\r\n",
      "        pass\r\n",
      "\r\n",
      "\r\n",
      "def mapper():\r\n",
      "    for row in sys.stdin:\r\n",
      "        key, value = row.split('\\t')\r\n",
      "        print(\"{}+{}\\t\".format(key, value.strip()))\r\n",
      "\r\n",
      "\r\n",
      "def reducer():\r\n",
      "    for _ in range(10):\r\n",
      "        key, _ = next(sys.stdin).split('\\t')\r\n",
      "        word, count = key.split(\"+\")\r\n",
      "        print(\"{}\\t{}\".format(word, count))\r\n",
      "    _rewind_stream(sys.stdin)\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    mr_command = sys.argv[1]\r\n",
      "    {\r\n",
      "        'map': mapper,\r\n",
      "        'reduce': reducer\r\n",
      "    }[mr_command]()\r\n"
     ]
    }
   ],
   "source": [
    "! cat top10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем команду на запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! find /usr/ -name hadoop-streaming.jar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/hadoop-mapreduce/hadoop-streaming.jar: symbolic link to hadoop-streaming-3.2.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "! file /usr/lib/hadoop-mapreduce/hadoop-streaming.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/result\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob7667778047204893517.jar tmpDir=null\n",
      "2023-01-24 16:00:55,046 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:00:55,266 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:00:55,305 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:00:55,306 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:00:55,506 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0002\n",
      "2023-01-24 16:00:55,866 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2023-01-24 16:00:56,357 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2023-01-24 16:00:56,924 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0002\n",
      "2023-01-24 16:00:56,925 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:00:57,096 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:00:57,097 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:00:57,181 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0002\n",
      "2023-01-24 16:00:57,244 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0002/\n",
      "2023-01-24 16:00:57,246 INFO mapreduce.Job: Running job: job_1674572529553_0002\n",
      "2023-01-24 16:01:02,362 INFO mapreduce.Job: Job job_1674572529553_0002 running in uber mode : false\n",
      "2023-01-24 16:01:02,363 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:01:13,775 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2023-01-24 16:01:14,789 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2023-01-24 16:01:16,893 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2023-01-24 16:01:17,949 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-01-24 16:01:22,018 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "2023-01-24 16:01:25,127 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2023-01-24 16:01:26,193 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2023-01-24 16:01:27,222 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "2023-01-24 16:01:28,269 INFO mapreduce.Job:  map 51% reduce 0%\n",
      "2023-01-24 16:01:31,401 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2023-01-24 16:01:33,477 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "2023-01-24 16:01:34,488 INFO mapreduce.Job:  map 59% reduce 7%\n",
      "2023-01-24 16:01:35,532 INFO mapreduce.Job:  map 68% reduce 7%\n",
      "2023-01-24 16:01:36,583 INFO mapreduce.Job:  map 68% reduce 14%\n",
      "2023-01-24 16:01:37,609 INFO mapreduce.Job:  map 68% reduce 21%\n",
      "2023-01-24 16:01:39,717 INFO mapreduce.Job:  map 70% reduce 21%\n",
      "2023-01-24 16:01:40,765 INFO mapreduce.Job:  map 70% reduce 22%\n",
      "2023-01-24 16:01:41,771 INFO mapreduce.Job:  map 78% reduce 22%\n",
      "2023-01-24 16:01:42,825 INFO mapreduce.Job:  map 81% reduce 24%\n",
      "2023-01-24 16:01:44,934 INFO mapreduce.Job:  map 86% reduce 24%\n",
      "2023-01-24 16:01:47,058 INFO mapreduce.Job:  map 89% reduce 26%\n",
      "2023-01-24 16:01:48,063 INFO mapreduce.Job:  map 89% reduce 28%\n",
      "2023-01-24 16:01:49,071 INFO mapreduce.Job:  map 95% reduce 29%\n",
      "2023-01-24 16:01:50,075 INFO mapreduce.Job:  map 97% reduce 29%\n",
      "2023-01-24 16:01:51,079 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "2023-01-24 16:01:53,088 INFO mapreduce.Job:  map 100% reduce 35%\n",
      "2023-01-24 16:01:54,093 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "2023-01-24 16:01:55,101 INFO mapreduce.Job:  map 100% reduce 54%\n",
      "2023-01-24 16:01:59,122 INFO mapreduce.Job:  map 100% reduce 64%\n",
      "2023-01-24 16:02:00,129 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "2023-01-24 16:02:01,134 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "2023-01-24 16:02:04,147 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "2023-01-24 16:02:06,157 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "2023-01-24 16:02:07,162 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:02:08,173 INFO mapreduce.Job: Job job_1674572529553_0002 completed successfully\n",
      "2023-01-24 16:02:08,254 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=16556486\n",
      "\t\tFILE: Number of bytes written=56973512\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1143550744\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=126\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=37\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=34\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=928962\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=408921\n",
      "\t\tTotal time spent by all map tasks (ms)=309654\n",
      "\t\tTotal time spent by all reduce tasks (ms)=136307\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=309654\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=136307\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=951257088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=418735104\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=41946754\n",
      "\t\tMap output bytes=313767903\n",
      "\t\tMap output materialized bytes=30752668\n",
      "\t\tInput split bytes=5597\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=30752668\n",
      "\t\tReduce input records=41946754\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=83893508\n",
      "\t\tShuffled Maps =111\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=111\n",
      "\t\tGC time elapsed (ms)=5297\n",
      "\t\tCPU time spent (ms)=191130\n",
      "\t\tPhysical memory (bytes) snapshot=13793001472\n",
      "\t\tVirtual memory (bytes) snapshot=173598027776\n",
      "\t\tTotal committed heap usage (bytes)=13442220032\n",
      "\t\tPeak Map Physical memory (bytes)=364077056\n",
      "\t\tPeak Map Virtual memory (bytes)=4346060800\n",
      "\t\tPeak Reduce Physical memory (bytes)=527900672\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4363419648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1143545147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2023-01-24 16:02:08,255 INFO streaming.StreamJob: Output directory: /user/tweets/result/\n",
      "CPU times: user 1.27 s, sys: 231 ms, total: 1.5 s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount.py \\\n",
    "-mapper \"python3 wordcount.py map\" \\\n",
    "-reducer \"python3 wordcount.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо того, что можно следить здесь в терминале, за выполнением можно наблюдать через UI-proxy в интерфейсе облака"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим результат\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2023-01-24 16:02 /user/tweets/result/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10107405 2023-01-24 16:02 /user/tweets/result/part-00000\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10134121 2023-01-24 16:02 /user/tweets/result/part-00001\r\n",
      "-rw-r--r--   1 ubuntu hadoop   10118293 2023-01-24 16:02 /user/tweets/result/part-00002\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\r\n",
      "aaaaa\t7\r\n",
      "aaaaaaaaaaaaaa\t3\r\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\r\n",
      "aaaaaaaaaaaaaaaaaaah\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\r\n",
      "aaaaaaaah\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaagh\t2\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result/part-* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим вторую задачу "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/top10\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob9174466269628992396.jar tmpDir=null\n",
      "2023-01-24 16:08:34,627 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:08:34,855 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:08:34,897 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:08:34,898 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:08:35,129 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0003\n",
      "2023-01-24 16:08:35,831 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2023-01-24 16:08:36,311 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2023-01-24 16:08:36,870 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0003\n",
      "2023-01-24 16:08:36,872 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:08:37,054 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:08:37,054 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:08:37,125 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0003\n",
      "2023-01-24 16:08:37,174 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0003/\n",
      "2023-01-24 16:08:37,175 INFO mapreduce.Job: Running job: job_1674572529553_0003\n",
      "2023-01-24 16:08:43,249 INFO mapreduce.Job: Job job_1674572529553_0003 running in uber mode : false\n",
      "2023-01-24 16:08:43,250 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:08:50,495 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2023-01-24 16:08:51,539 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2023-01-24 16:08:52,632 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2023-01-24 16:08:53,674 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-01-24 16:08:57,823 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2023-01-24 16:08:59,912 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2023-01-24 16:09:00,972 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2023-01-24 16:09:03,092 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2023-01-24 16:09:04,156 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2023-01-24 16:09:05,208 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "2023-01-24 16:09:07,316 INFO mapreduce.Job:  map 73% reduce 24%\n",
      "2023-01-24 16:09:08,352 INFO mapreduce.Job:  map 77% reduce 24%\n",
      "2023-01-24 16:09:09,412 INFO mapreduce.Job:  map 80% reduce 24%\n",
      "2023-01-24 16:09:10,417 INFO mapreduce.Job:  map 90% reduce 24%\n",
      "2023-01-24 16:09:12,426 INFO mapreduce.Job:  map 97% reduce 30%\n",
      "2023-01-24 16:09:13,431 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2023-01-24 16:09:18,453 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "2023-01-24 16:09:19,458 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:09:19,465 INFO mapreduce.Job: Job job_1674572529553_0003 completed successfully\n",
      "2023-01-24 16:09:19,540 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13564266\n",
      "\t\tFILE: Number of bytes written=35393708\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31346018\n",
      "\t\tHDFS: Number of bytes written=106\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=20\n",
      "\t\tRack-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=466803\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=78114\n",
      "\t\tTotal time spent by all map tasks (ms)=155601\n",
      "\t\tTotal time spent by all reduce tasks (ms)=26038\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=155601\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=26038\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=478006272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=79988736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=14323416\n",
      "\t\tInput split bytes=4230\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=14323416\n",
      "\t\tReduce input records=2831736\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=5663472\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=4211\n",
      "\t\tCPU time spent (ms)=92670\n",
      "\t\tPhysical memory (bytes) snapshot=13053054976\n",
      "\t\tVirtual memory (bytes) snapshot=134553300992\n",
      "\t\tTotal committed heap usage (bytes)=13157531648\n",
      "\t\tPeak Map Physical memory (bytes)=505073664\n",
      "\t\tPeak Map Virtual memory (bytes)=4346241024\n",
      "\t\tPeak Reduce Physical memory (bytes)=524472320\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4358909952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31341788\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=106\n",
      "2023-01-24 16:09:19,541 INFO streaming.StreamJob: Output directory: /user/tweets/top10/\n",
      "CPU times: user 805 ms, sys: 144 ms, total: 949 ms\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10/\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10.py \\\n",
    "-mapper \"python top10.py map\" \\\n",
    "-reducer \"python top10.py reduce\" \\\n",
    "-input /user/tweets/result/ \\\n",
    "-output /user/tweets/top10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2023-01-24 16:09 /user/tweets/top10/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop        106 2023-01-24 16:09 /user/tweets/top10/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t3015051\r\n",
      "co\t2833375\r\n",
      "https\t2454132\r\n",
      "the\t591885\r\n",
      "to\t589004\r\n",
      "in\t457433\r\n",
      "a\t412888\r\n",
      "s\t397889\r\n",
      "http\t375299\r\n",
      "of\t350983\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10/part-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed cache\n",
    "\n",
    "Помимо самого скрипта, мы можем положить в MapReduce любой другой файл, который может пригодиться для работы программы. Например при подсчете количества слов мы бы хотели выкинуть \"стоп-слова\". Их количество скорее всего не очень большое поэтому смело может передавать их обычным файлом. Hadoop гарантирует, что доставит все файлы ко всем машинам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10/part-* > stop-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Хозяйке на заметку\n",
    "\n",
    "В питоне уже есть хорошая стандартная библиотека, которая позволяет вам гораздо удобнее работать с такимим стримовыми данными. Давайте напишем новую задачу со стоп словами, чтобы они смотрелись поприличнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount2.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            print(\"{}\\t{}\".format(word, 1))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(int(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10-2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10-2.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def build_stop_words():\n",
    "    with open('stop-words.txt', 'r') as f:\n",
    "        stop_words = {x.split('\\t')[0] for x in f}\n",
    "    return stop_words\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(word, count.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\t3015051\r\n",
      "co\t2833375\r\n",
      "https\t2454132\r\n",
      "the\t591885\r\n",
      "to\t589004\r\n",
      "in\t457433\r\n",
      "a\t412888\r\n",
      "s\t397889\r\n",
      "http\t375299\r\n",
      "of\t350983\r\n"
     ]
    }
   ],
   "source": [
    "! cat stop-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/top10-stop-words\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob2658700216274696416.jar tmpDir=null\n",
      "2023-01-24 16:19:32,776 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:19:33,004 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:19:33,047 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:19:33,048 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:19:33,239 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0004\n",
      "2023-01-24 16:19:33,597 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2023-01-24 16:19:34,103 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2023-01-24 16:19:34,660 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0004\n",
      "2023-01-24 16:19:34,661 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:19:34,895 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:19:34,895 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:19:34,965 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0004\n",
      "2023-01-24 16:19:35,006 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0004/\n",
      "2023-01-24 16:19:35,008 INFO mapreduce.Job: Running job: job_1674572529553_0004\n",
      "2023-01-24 16:19:40,099 INFO mapreduce.Job: Job job_1674572529553_0004 running in uber mode : false\n",
      "2023-01-24 16:19:40,101 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:19:48,392 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2023-01-24 16:19:50,468 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2023-01-24 16:19:51,536 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-01-24 16:19:52,543 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-01-24 16:19:54,593 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2023-01-24 16:19:57,625 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-01-24 16:19:58,630 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2023-01-24 16:20:00,642 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "2023-01-24 16:20:03,665 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "2023-01-24 16:20:04,671 INFO mapreduce.Job:  map 73% reduce 22%\n",
      "2023-01-24 16:20:05,678 INFO mapreduce.Job:  map 80% reduce 22%\n",
      "2023-01-24 16:20:06,683 INFO mapreduce.Job:  map 83% reduce 22%\n",
      "2023-01-24 16:20:10,705 INFO mapreduce.Job:  map 87% reduce 28%\n",
      "2023-01-24 16:20:11,709 INFO mapreduce.Job:  map 93% reduce 28%\n",
      "2023-01-24 16:20:12,713 INFO mapreduce.Job:  map 97% reduce 28%\n",
      "2023-01-24 16:20:13,718 INFO mapreduce.Job:  map 100% reduce 28%\n",
      "2023-01-24 16:20:16,732 INFO mapreduce.Job:  map 100% reduce 58%\n",
      "2023-01-24 16:20:20,752 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:20:21,765 INFO mapreduce.Job: Job job_1674572529553_0004 completed successfully\n",
      "2023-01-24 16:20:21,848 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=13564266\n",
      "\t\tFILE: Number of bytes written=35403349\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31346018\n",
      "\t\tHDFS: Number of bytes written=109\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=24\n",
      "\t\tRack-local map tasks=6\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=482868\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=89967\n",
      "\t\tTotal time spent by all map tasks (ms)=160956\n",
      "\t\tTotal time spent by all reduce tasks (ms)=29989\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=160956\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=29989\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=494456832\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=92126208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=14323416\n",
      "\t\tInput split bytes=4230\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=14323416\n",
      "\t\tReduce input records=2831736\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=5663472\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=4113\n",
      "\t\tCPU time spent (ms)=87120\n",
      "\t\tPhysical memory (bytes) snapshot=13193736192\n",
      "\t\tVirtual memory (bytes) snapshot=134540402688\n",
      "\t\tTotal committed heap usage (bytes)=13423869952\n",
      "\t\tPeak Map Physical memory (bytes)=496570368\n",
      "\t\tPeak Map Virtual memory (bytes)=4348149760\n",
      "\t\tPeak Reduce Physical memory (bytes)=510095360\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4343947264\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31341788\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=109\n",
      "2023-01-24 16:20:21,848 INFO streaming.StreamJob: Output directory: /user/tweets/top10-stop-words/\n",
      "CPU times: user 879 ms, sys: 156 ms, total: 1.03 s\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10-stop-words/ || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"top-10-stop-words\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files top10-2.py,stop-words.txt \\\n",
    "-mapper \"python top10-2.py map\" \\\n",
    "-reducer \"python top10-2.py reduce\" \\\n",
    "-input /user/tweets/result/ \\\n",
    "-output /user/tweets/top10-stop-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t287232\r\n",
      "for\t272995\r\n",
      "and\t247749\r\n",
      "is\t246856\r\n",
      "on\t210172\r\n",
      "you\t196950\r\n",
      "trump\t169520\r\n",
      "news\t156101\r\n",
      "it\t152816\r\n",
      "with\t134178\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10-stop-words/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ускоряем вычисления \n",
    "\n",
    "Несмотря на все оптимизации внутри Hadoop, самое узкое место - это передача данных от mapper к reducer. Таким образом если у нас получиться ускорить передачи данных в этом месте, мы сможем сильно ускорить весь процесс ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wordcount3.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    counter = Counter()\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            counter[word] += 1\n",
    "    \n",
    "    for word, number in counter.items():\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(int(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/result-fast1\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob5383784256749372209.jar tmpDir=null\n",
      "2023-01-24 16:26:29,085 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:26:29,322 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:26:29,357 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:26:29,358 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:26:29,568 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0005\n",
      "2023-01-24 16:26:29,872 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2023-01-24 16:26:30,398 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2023-01-24 16:26:30,974 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0005\n",
      "2023-01-24 16:26:30,976 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:26:31,150 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:26:31,150 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:26:31,218 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0005\n",
      "2023-01-24 16:26:31,260 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0005/\n",
      "2023-01-24 16:26:31,262 INFO mapreduce.Job: Running job: job_1674572529553_0005\n",
      "2023-01-24 16:26:36,332 INFO mapreduce.Job: Job job_1674572529553_0005 running in uber mode : false\n",
      "2023-01-24 16:26:36,333 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:26:45,710 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2023-01-24 16:26:47,812 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-01-24 16:26:51,941 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "2023-01-24 16:26:52,967 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2023-01-24 16:26:57,126 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2023-01-24 16:26:58,133 INFO mapreduce.Job:  map 59% reduce 0%\n",
      "2023-01-24 16:27:06,170 INFO mapreduce.Job:  map 70% reduce 0%\n",
      "2023-01-24 16:27:08,179 INFO mapreduce.Job:  map 78% reduce 8%\n",
      "2023-01-24 16:27:09,184 INFO mapreduce.Job:  map 84% reduce 24%\n",
      "2023-01-24 16:27:11,196 INFO mapreduce.Job:  map 86% reduce 24%\n",
      "2023-01-24 16:27:14,211 INFO mapreduce.Job:  map 86% reduce 26%\n",
      "2023-01-24 16:27:15,217 INFO mapreduce.Job:  map 95% reduce 29%\n",
      "2023-01-24 16:27:16,222 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "2023-01-24 16:27:20,396 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "2023-01-24 16:27:23,486 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "2023-01-24 16:27:24,492 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:27:24,501 INFO mapreduce.Job: Job job_1674572529553_0005 completed successfully\n",
      "2023-01-24 16:27:24,580 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17763203\n",
      "\t\tFILE: Number of bytes written=55442549\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1143550744\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=126\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=37\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=35\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=877329\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=275676\n",
      "\t\tTotal time spent by all map tasks (ms)=292443\n",
      "\t\tTotal time spent by all reduce tasks (ms)=91892\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=292443\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=91892\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=898384896\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=282292224\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=5204737\n",
      "\t\tMap output bytes=53033060\n",
      "\t\tMap output materialized bytes=28014548\n",
      "\t\tInput split bytes=5597\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=28014548\n",
      "\t\tReduce input records=5204737\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=10409474\n",
      "\t\tShuffled Maps =111\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=111\n",
      "\t\tGC time elapsed (ms)=6179\n",
      "\t\tCPU time spent (ms)=98840\n",
      "\t\tPhysical memory (bytes) snapshot=12899434496\n",
      "\t\tVirtual memory (bytes) snapshot=173566951424\n",
      "\t\tTotal committed heap usage (bytes)=12648972288\n",
      "\t\tPeak Map Physical memory (bytes)=367157248\n",
      "\t\tPeak Map Virtual memory (bytes)=4340801536\n",
      "\t\tPeak Reduce Physical memory (bytes)=271810560\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4364795904\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1143545147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2023-01-24 16:27:24,581 INFO streaming.StreamJob: Output directory: /user/tweets/result-fast1/\n",
      "CPU times: user 995 ms, sys: 151 ms, total: 1.15 s\n",
      "Wall time: 60 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result-fast1 || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount3.py \\\n",
    "-mapper \"python3 wordcount3.py map\" \\\n",
    "-reducer \"python3 wordcount3.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result-fast1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\r\n",
      "aaaaa\t7\r\n",
      "aaaaaaaaaaaaaa\t3\r\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\r\n",
      "aaaaaaaaaaaaaaaaaaah\t1\r\n",
      "aaaaaaaaaaaaand\t1\r\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\r\n",
      "aaaaaaaah\t1\r\n",
      "aaaaaaaamen\t1\r\n",
      "aaaaaaagh\t2\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result-fast1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако у этого решения есть **очень большой минус** - сложность по памяти **O(n)**. Это означает, что вычисление может упасть если данные попадутся неудачные. \n",
    "\n",
    "Важный принцип работы с большими данными - все алгоритмы должны работать меньше чем за O(n). Это относится не только к MapReduce, а в целом почти к любым инструментам обработки больших данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hdfs dfs -ls /user/tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем комбайнер\n",
    "\n",
    "Чтобы побороться с этой бедой, воспользуемся дополнительным инструментом в Hadoop - Combiner. По сути это маленький Reduce, который запускается после маппера. Это позволяет уменьшить количество выходных данных с Map стадии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://habrastorage.org/getpro/habr/post_images/587/2d2/dfe/5872d2dfe12643665370708d225bc1d4.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/result-fast2\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob2036465758215084331.jar tmpDir=null\n",
      "2023-01-24 16:30:44,361 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:30:44,589 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:30:44,629 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:30:44,630 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:30:44,863 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0006\n",
      "2023-01-24 16:30:45,190 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2023-01-24 16:30:45,284 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2023-01-24 16:30:45,857 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0006\n",
      "2023-01-24 16:30:45,859 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:30:46,024 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:30:46,024 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:30:46,111 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0006\n",
      "2023-01-24 16:30:46,193 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0006/\n",
      "2023-01-24 16:30:46,196 INFO mapreduce.Job: Running job: job_1674572529553_0006\n",
      "2023-01-24 16:30:51,271 INFO mapreduce.Job: Job job_1674572529553_0006 running in uber mode : false\n",
      "2023-01-24 16:30:51,272 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:31:03,782 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2023-01-24 16:31:05,829 INFO mapreduce.Job:  map 11% reduce 0%\n",
      "2023-01-24 16:31:06,888 INFO mapreduce.Job:  map 19% reduce 0%\n",
      "2023-01-24 16:31:07,950 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-01-24 16:31:13,191 INFO mapreduce.Job:  map 32% reduce 0%\n",
      "2023-01-24 16:31:14,197 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2023-01-24 16:31:17,219 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2023-01-24 16:31:18,224 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2023-01-24 16:31:19,230 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2023-01-24 16:31:22,245 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "2023-01-24 16:31:23,249 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2023-01-24 16:31:24,256 INFO mapreduce.Job:  map 57% reduce 6%\n",
      "2023-01-24 16:31:25,262 INFO mapreduce.Job:  map 57% reduce 12%\n",
      "2023-01-24 16:31:26,267 INFO mapreduce.Job:  map 62% reduce 12%\n",
      "2023-01-24 16:31:28,278 INFO mapreduce.Job:  map 62% reduce 19%\n",
      "2023-01-24 16:31:30,287 INFO mapreduce.Job:  map 65% reduce 20%\n",
      "2023-01-24 16:31:31,292 INFO mapreduce.Job:  map 65% reduce 21%\n",
      "2023-01-24 16:31:33,301 INFO mapreduce.Job:  map 70% reduce 21%\n",
      "2023-01-24 16:31:34,306 INFO mapreduce.Job:  map 73% reduce 21%\n",
      "2023-01-24 16:31:35,312 INFO mapreduce.Job:  map 76% reduce 21%\n",
      "2023-01-24 16:31:36,316 INFO mapreduce.Job:  map 84% reduce 23%\n",
      "2023-01-24 16:31:37,320 INFO mapreduce.Job:  map 84% reduce 25%\n",
      "2023-01-24 16:31:38,326 INFO mapreduce.Job:  map 86% reduce 25%\n",
      "2023-01-24 16:31:40,335 INFO mapreduce.Job:  map 89% reduce 27%\n",
      "2023-01-24 16:31:42,343 INFO mapreduce.Job:  map 92% reduce 29%\n",
      "2023-01-24 16:31:43,347 INFO mapreduce.Job:  map 92% reduce 30%\n",
      "2023-01-24 16:31:44,352 INFO mapreduce.Job:  map 95% reduce 30%\n",
      "2023-01-24 16:31:45,357 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2023-01-24 16:31:46,363 INFO mapreduce.Job:  map 100% reduce 31%\n",
      "2023-01-24 16:31:48,372 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "2023-01-24 16:31:49,376 INFO mapreduce.Job:  map 100% reduce 60%\n",
      "2023-01-24 16:31:50,382 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "2023-01-24 16:31:51,386 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:31:51,395 INFO mapreduce.Job: Job job_1674572529553_0006 completed successfully\n",
      "2023-01-24 16:31:51,473 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17763203\n",
      "\t\tFILE: Number of bytes written=55457469\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1143550744\n",
      "\t\tHDFS: Number of bytes written=30359819\n",
      "\t\tHDFS: Number of read operations=126\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=38\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=35\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1124493\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=359268\n",
      "\t\tTotal time spent by all map tasks (ms)=374831\n",
      "\t\tTotal time spent by all reduce tasks (ms)=119756\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=374831\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=119756\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=1151480832\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=367890432\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=41946754\n",
      "\t\tMap output bytes=313767903\n",
      "\t\tMap output materialized bytes=28014548\n",
      "\t\tInput split bytes=5597\n",
      "\t\tCombine input records=41946754\n",
      "\t\tCombine output records=5204737\n",
      "\t\tReduce input groups=2831736\n",
      "\t\tReduce shuffle bytes=28014548\n",
      "\t\tReduce input records=5204737\n",
      "\t\tReduce output records=2831736\n",
      "\t\tSpilled Records=10409474\n",
      "\t\tShuffled Maps =111\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=111\n",
      "\t\tGC time elapsed (ms)=5316\n",
      "\t\tCPU time spent (ms)=176260\n",
      "\t\tPhysical memory (bytes) snapshot=13217419264\n",
      "\t\tVirtual memory (bytes) snapshot=173577719808\n",
      "\t\tTotal committed heap usage (bytes)=14345568256\n",
      "\t\tPeak Map Physical memory (bytes)=369971200\n",
      "\t\tPeak Map Virtual memory (bytes)=4341882880\n",
      "\t\tPeak Reduce Physical memory (bytes)=294920192\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4362960896\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1143545147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=30359819\n",
      "2023-01-24 16:31:51,473 INFO streaming.StreamJob: Output directory: /user/tweets/result-fast2/\n",
      "CPU times: user 1.18 s, sys: 180 ms, total: 1.36 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/result-fast2 || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/wordcount2.py \\\n",
    "-mapper \"python3 wordcount2.py map\" \\\n",
    "-combiner \"python3 wordcount2.py reduce\" \\\n",
    "-reducer \"python3 wordcount2.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/result-fast2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\t1726\n",
      "aaaaa\t7\n",
      "aaaaaaaaaaaaaa\t3\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddddddddd\t1\n",
      "aaaaaaaaaaaaaaaaaaah\t1\n",
      "aaaaaaaaaaaaand\t1\n",
      "aaaaaaaaannnnnnnnnnnddddddddddddd\t1\n",
      "aaaaaaaah\t1\n",
      "aaaaaaaamen\t1\n",
      "aaaaaaagh\t2\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/result-fast1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто combiner может просто совпадать с reducer однако это не всегда так по следующей причине - combiner не имеет права менять формат вывода после стадии map.\n",
    "\n",
    "Hadoop самостоятельно опеределяет целесообразность запуска combiner и может его не запускать вовсе.\n",
    "Или например задача может вообще не подходить под такую модель запуска. Если мы ищем среднее, то нельзя заранее подсчитывать среднее на стадии combiner - макмимум, что мы там можем запустить - это подсчет количество и суммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10-3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top10-3.py\n",
    "\n",
    "import sys\n",
    "import collections\n",
    "from itertools import islice\n",
    "\n",
    "def build_stop_words():\n",
    "    with open('stop-words.txt', 'r') as f:\n",
    "        stop_words = {x.split('\\t')[0] for x in f}\n",
    "    return stop_words\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def rewind():\n",
    "    collections.deque(sys.stdin, maxlen=0)\n",
    "\n",
    "def mapper():\n",
    "    for key, value in kv_stream():\n",
    "        print(\"{}+{}\\t\".format(key, value.strip()))\n",
    "\n",
    "def reducer():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}\\t{}\".format(word, count.strip()))\n",
    "    rewind()\n",
    "    \n",
    "def combiner():\n",
    "    stop_words = build_stop_words()\n",
    "    first_10_stream = islice(filter(lambda x: x[0] not in stop_words, kv_stream('+')), 10)\n",
    "    \n",
    "    for word, count in first_10_stream:\n",
    "        print(\"{}+{}\\t\".format(word, count.strip()))\n",
    "    rewind()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer,\n",
    "        'combiner': combiner\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/top10-fast\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob2343487677005611575.jar tmpDir=null\n",
      "2023-01-24 16:33:22,355 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:33:22,566 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:33:22,604 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:33:22,606 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:33:22,828 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0007\n",
      "2023-01-24 16:33:23,584 INFO mapred.FileInputFormat: Total input files to process : 3\n",
      "2023-01-24 16:33:23,667 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2023-01-24 16:33:24,229 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0007\n",
      "2023-01-24 16:33:24,231 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:33:24,405 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:33:24,406 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:33:24,472 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0007\n",
      "2023-01-24 16:33:24,501 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0007/\n",
      "2023-01-24 16:33:24,502 INFO mapreduce.Job: Running job: job_1674572529553_0007\n",
      "2023-01-24 16:33:29,572 INFO mapreduce.Job: Job job_1674572529553_0007 running in uber mode : false\n",
      "2023-01-24 16:33:29,574 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:33:36,664 INFO mapreduce.Job:  map 3% reduce 0%\n",
      "2023-01-24 16:33:39,745 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2023-01-24 16:33:40,753 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2023-01-24 16:33:43,768 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-01-24 16:33:46,792 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2023-01-24 16:33:47,799 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2023-01-24 16:33:48,805 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "2023-01-24 16:33:49,810 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2023-01-24 16:33:52,825 INFO mapreduce.Job:  map 60% reduce 0%\n",
      "2023-01-24 16:33:54,835 INFO mapreduce.Job:  map 63% reduce 20%\n",
      "2023-01-24 16:33:55,857 INFO mapreduce.Job:  map 67% reduce 20%\n",
      "2023-01-24 16:33:56,897 INFO mapreduce.Job:  map 73% reduce 20%\n",
      "2023-01-24 16:33:57,965 INFO mapreduce.Job:  map 83% reduce 20%\n",
      "2023-01-24 16:33:58,978 INFO mapreduce.Job:  map 87% reduce 20%\n",
      "2023-01-24 16:33:59,986 INFO mapreduce.Job:  map 93% reduce 20%\n",
      "2023-01-24 16:34:00,991 INFO mapreduce.Job:  map 93% reduce 30%\n",
      "2023-01-24 16:34:01,995 INFO mapreduce.Job:  map 100% reduce 30%\n",
      "2023-01-24 16:34:02,999 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:34:04,010 INFO mapreduce.Job: Job job_1674572529553_0007 completed successfully\n",
      "2023-01-24 16:34:04,087 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2185\n",
      "\t\tFILE: Number of bytes written=7533150\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=31346198\n",
      "\t\tHDFS: Number of bytes written=109\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=31\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=15\n",
      "\t\tRack-local map tasks=16\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=482061\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=62595\n",
      "\t\tTotal time spent by all map tasks (ms)=160687\n",
      "\t\tTotal time spent by all reduce tasks (ms)=20865\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=160687\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=20865\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=493630464\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=64097280\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2831736\n",
      "\t\tMap output records=2831736\n",
      "\t\tMap output bytes=33191556\n",
      "\t\tMap output materialized bytes=3766\n",
      "\t\tInput split bytes=4410\n",
      "\t\tCombine input records=2831736\n",
      "\t\tCombine output records=300\n",
      "\t\tReduce input groups=300\n",
      "\t\tReduce shuffle bytes=3766\n",
      "\t\tReduce input records=300\n",
      "\t\tReduce output records=10\n",
      "\t\tSpilled Records=600\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=3859\n",
      "\t\tCPU time spent (ms)=84310\n",
      "\t\tPhysical memory (bytes) snapshot=12634013696\n",
      "\t\tVirtual memory (bytes) snapshot=134578229248\n",
      "\t\tTotal committed heap usage (bytes)=12844531712\n",
      "\t\tPeak Map Physical memory (bytes)=478486528\n",
      "\t\tPeak Map Virtual memory (bytes)=4354662400\n",
      "\t\tPeak Reduce Physical memory (bytes)=209358848\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4345192448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31341788\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=109\n",
      "2023-01-24 16:34:04,088 INFO streaming.StreamJob: Output directory: /user/tweets/top10-fast/\n",
      "CPU times: user 744 ms, sys: 137 ms, total: 882 ms\n",
      "Wall time: 45.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "! hdfs dfs -rm -r /user/tweets/top10-fast || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"word-count\" \\\n",
    "-D mapreduce.job.reduces=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-files ~/top10-3.py,stop-words.txt \\\n",
    "-mapper \"python3 top10-3.py map\" \\\n",
    "-combiner \"python3 top10-3.py combiner\" \\\n",
    "-reducer \"python3 top10-3.py reduce\" \\\n",
    "-input /user/tweets/result-fast1 \\\n",
    "-output /user/tweets/top10-fast/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\t287232\r\n",
      "for\t272995\r\n",
      "and\t247749\r\n",
      "is\t246856\r\n",
      "on\t210172\r\n",
      "you\t196950\r\n",
      "trump\t169520\r\n",
      "news\t156101\r\n",
      "it\t152816\r\n",
      "with\t134178\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/top10-fast/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомный Partitioner\n",
    "\n",
    "В Hadoop MapReduce можно указывать свой кастомный партишенер, который будет определять, как разбивать данные по редюсерам.\n",
    "\n",
    "Это бывает важно, когда вы используете сложный ключ и много редюсеров - вполне возможно вы захотите, чтобы такие ключи определенным образом распределялись по редюс-задачам.\n",
    "\n",
    "Для наглядности давайте решим такую задачу - для каждого пользователя подсчитаем, на каких языках он писал твиты и в каком количестве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lang-distribution.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lang-distribution.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "from collections import Counter\n",
    "from itertools import groupby\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def mapper():\n",
    "    for row in csv_stream():\n",
    "        author, lang = row[1], row[4]\n",
    "        print(\"{}+{}\\t1\".format(author.strip(), lang.strip()))\n",
    "\n",
    "def reducer():\n",
    "    for author, records in groupby(kv_stream('+'), lambda x: x[0]):\n",
    "        langs_stream = (x.split('\\t') for _, x in records)\n",
    "        for lang, group in groupby(langs_stream, lambda x: x[0]):\n",
    "            count = sum(int(x) for _, x in group)\n",
    "            print(\"{}+{}\\t{}\".format(author, lang, count))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer,\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры\n",
    "\n",
    "```\n",
    "-D mapred.partitioner.class=org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "```\n",
    "\n",
    "Указывают на то, что разделятся на редюсеры записи должны не по полному ключу, а только по первой его части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/lang-dist\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob1335278815778382587.jar tmpDir=null\n",
      "2023-01-24 16:40:15,086 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:40:15,310 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:40:15,345 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:40:15,346 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:40:15,561 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0008\n",
      "2023-01-24 16:40:16,277 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2023-01-24 16:40:16,357 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2023-01-24 16:40:16,528 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0008\n",
      "2023-01-24 16:40:16,529 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:40:16,731 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:40:16,731 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:40:16,813 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0008\n",
      "2023-01-24 16:40:16,841 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0008/\n",
      "2023-01-24 16:40:16,842 INFO mapreduce.Job: Running job: job_1674572529553_0008\n",
      "2023-01-24 16:40:21,908 INFO mapreduce.Job: Job job_1674572529553_0008 running in uber mode : false\n",
      "2023-01-24 16:40:21,909 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:40:30,161 INFO mapreduce.Job:  map 8% reduce 0%\n",
      "2023-01-24 16:40:31,328 INFO mapreduce.Job:  map 14% reduce 0%\n",
      "2023-01-24 16:40:32,356 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2023-01-24 16:40:36,501 INFO mapreduce.Job:  map 38% reduce 0%\n",
      "2023-01-24 16:40:37,506 INFO mapreduce.Job:  map 41% reduce 0%\n",
      "2023-01-24 16:40:38,533 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "2023-01-24 16:40:39,596 INFO mapreduce.Job:  map 49% reduce 0%\n",
      "2023-01-24 16:40:40,636 INFO mapreduce.Job:  map 54% reduce 0%\n",
      "2023-01-24 16:40:42,711 INFO mapreduce.Job:  map 57% reduce 0%\n",
      "2023-01-24 16:40:43,720 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "2023-01-24 16:40:47,784 INFO mapreduce.Job:  map 68% reduce 0%\n",
      "2023-01-24 16:40:48,799 INFO mapreduce.Job:  map 76% reduce 0%\n",
      "2023-01-24 16:40:49,803 INFO mapreduce.Job:  map 81% reduce 8%\n",
      "2023-01-24 16:40:50,809 INFO mapreduce.Job:  map 81% reduce 17%\n",
      "2023-01-24 16:40:51,814 INFO mapreduce.Job:  map 86% reduce 17%\n",
      "2023-01-24 16:40:53,825 INFO mapreduce.Job:  map 89% reduce 27%\n",
      "2023-01-24 16:40:55,835 INFO mapreduce.Job:  map 100% reduce 29%\n",
      "2023-01-24 16:40:56,847 INFO mapreduce.Job:  map 100% reduce 41%\n",
      "2023-01-24 16:40:58,855 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:40:58,863 INFO mapreduce.Job: Job job_1674572529553_0008 completed successfully\n",
      "2023-01-24 16:40:58,939 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=359106\n",
      "\t\tFILE: Number of bytes written=10429952\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1143550744\n",
      "\t\tHDFS: Number of bytes written=380326\n",
      "\t\tHDFS: Number of read operations=126\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=9\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=37\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tData-local map tasks=35\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=651462\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=204279\n",
      "\t\tTotal time spent by all map tasks (ms)=217154\n",
      "\t\tTotal time spent by all reduce tasks (ms)=68093\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=217154\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=68093\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=667097088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=209181696\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=2946207\n",
      "\t\tMap output records=2946207\n",
      "\t\tMap output bytes=67013403\n",
      "\t\tMap output materialized bytes=365980\n",
      "\t\tInput split bytes=5597\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=16091\n",
      "\t\tReduce shuffle bytes=365980\n",
      "\t\tReduce input records=2946207\n",
      "\t\tReduce output records=16091\n",
      "\t\tSpilled Records=5892414\n",
      "\t\tShuffled Maps =111\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=111\n",
      "\t\tGC time elapsed (ms)=6004\n",
      "\t\tCPU time spent (ms)=82900\n",
      "\t\tPhysical memory (bytes) snapshot=14166401024\n",
      "\t\tVirtual memory (bytes) snapshot=173593284608\n",
      "\t\tTotal committed heap usage (bytes)=14907080704\n",
      "\t\tPeak Map Physical memory (bytes)=382377984\n",
      "\t\tPeak Map Virtual memory (bytes)=4342747136\n",
      "\t\tPeak Reduce Physical memory (bytes)=397230080\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4344201216\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1143545147\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=380326\n",
      "2023-01-24 16:40:58,940 INFO streaming.StreamJob: Output directory: /user/tweets/lang-dist\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/lang-dist || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"lang-dist\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "-D mapreduce.map.output.key.field.separator='+' \\\n",
    "-D mapred.partitioner.class=org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-files ~/lang-distribution.py \\\n",
    "-mapper \"python3 lang-distribution.py map\" \\\n",
    "-reducer \"python3 lang-distribution.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/lang-dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1488REASONS+Russian\t50\n",
      "1488REASONS+Serbian\t1\n",
      "1488REASONS+Ukrainian\t1\n",
      "1D_NICOLE_+Albanian\t1\n",
      "1D_NICOLE_+English\t41\n",
      "1D_NICOLE_+Tagalog (Filipino)\t2\n",
      "1ERIK_LEE+English\t2\n",
      "459JISALGE+Russian\t1\n",
      "4MYSQUAD+Arabic\t5\n",
      "4MYSQUAD+Catalan\t1\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /user/tweets/lang-dist/* | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как дебажить ошибки\n",
    "\n",
    "Через yarn можно выводить логи приложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mistake.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mistake.py\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def csv_stream():\n",
    "    return csv.reader(iter(sys.stdin.readline, ''))\n",
    "\n",
    "def kv_stream(sep=\"\\t\"):\n",
    "    return map(lambda x: x.split(sep), sys.stdin)\n",
    "\n",
    "\n",
    "def mapper():\n",
    "    pattern = re.compile(r\"[a-z]+\")\n",
    "    for row in csv_stream():\n",
    "        content = row[2]\n",
    "        for match in pattern.finditer(content.lower()):\n",
    "            word = match.group(0)\n",
    "            strange_number = 1.0 / (len(word) - 1)\n",
    "            print(\"{}\\t{}\".format(word, strange_number))\n",
    "\n",
    "\n",
    "def reducer():\n",
    "    for key, group in groupby(kv_stream(), lambda x: x[0]):\n",
    "        word = key\n",
    "        number = sum(float(x) for _, x in group)\n",
    "        print(\"{}\\t{}\".format(word, number))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mr_command = sys.argv[1]\n",
    "    {\n",
    "        'map': mapper,\n",
    "        'reduce': reducer\n",
    "    }[mr_command]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/tweets/mistake\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob472224327129279597.jar tmpDir=null\n",
      "2023-01-24 16:41:58,494 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:41:58,725 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:41:58,768 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:41:58,769 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "2023-01-24 16:41:58,965 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1674572529553_0009\n",
      "2023-01-24 16:41:59,714 INFO mapred.FileInputFormat: Total input files to process : 13\n",
      "2023-01-24 16:42:00,217 INFO mapreduce.JobSubmitter: number of splits:37\n",
      "2023-01-24 16:42:00,796 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1674572529553_0009\n",
      "2023-01-24 16:42:00,797 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-01-24 16:42:00,980 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-01-24 16:42:00,980 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-01-24 16:42:01,058 INFO impl.YarnClientImpl: Submitted application application_1674572529553_0009\n",
      "2023-01-24 16:42:01,092 INFO mapreduce.Job: The url to track the job: http://rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net:8088/proxy/application_1674572529553_0009/\n",
      "2023-01-24 16:42:01,100 INFO mapreduce.Job: Running job: job_1674572529553_0009\n",
      "2023-01-24 16:42:06,176 INFO mapreduce.Job: Job job_1674572529553_0009 running in uber mode : false\n",
      "2023-01-24 16:42:06,177 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-01-24 16:42:10,344 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:10,356 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000006_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:10,364 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000000_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,388 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000013_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,389 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000003_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,390 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000005_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,392 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000017_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,393 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000012_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,394 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000002_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,395 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000004_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,397 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000016_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:12,399 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000007_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:15,441 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,508 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000017_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,511 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000020_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,517 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000021_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,523 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000005_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,527 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000031_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:17,529 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000013_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,540 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000006_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,542 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000012_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,544 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000003_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,545 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000000_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:18,547 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000002_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:20,613 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000004_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:21,676 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000006_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:22,684 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000005_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,729 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000016_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,745 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000017_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,760 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000021_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,784 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000013_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,813 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000007_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,829 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000020_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,850 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:23,854 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000031_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:24,897 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000012_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:26,998 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000000_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:27,004 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000003_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:28,022 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000021_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:30,095 INFO mapreduce.Job: Task Id : attempt_1674572529553_0009_m_000002_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:326)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:539)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n",
      "\n",
      "2023-01-24 16:42:31,099 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-01-24 16:42:33,116 INFO mapreduce.Job: Job job_1674572529553_0009 failed with state FAILED due to: Task failed task_1674572529553_0009_m_000017\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n",
      "\n",
      "2023-01-24 16:42:33,190 INFO mapreduce.Job: Counters: 14\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=41\n",
      "\t\tKilled map tasks=36\n",
      "\t\tKilled reduce tasks=3\n",
      "\t\tLaunched map tasks=50\n",
      "\t\tOther local map tasks=35\n",
      "\t\tData-local map tasks=15\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=612042\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=204014\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=204014\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=626731008\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "2023-01-24 16:42:33,190 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r /user/tweets/mistake || true\n",
    "! yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-D mapreduce.job.name=\"mistake\" \\\n",
    "-D mapreduce.job.reduces=3 \\\n",
    "-files ~/mistake.py \\\n",
    "-mapper \"python3 mistake.py map\" \\\n",
    "-reducer \"python3 mistake.py reduce\" \\\n",
    "-input /user/tweets/data/ \\\n",
    "-output /user/tweets/mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-24 16:43:07,268 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:43:07,551 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "Container: container_1674572529553_0009_01_000015 on rc1a-dataproc-d-1jipfldazn1nw9u1.mdb.yandexcloud.net_33887\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Tue Jan 24 16:42:38 +0000 2023\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n",
      "\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n",
      "\n",
      "Container: container_1674572529553_0009_01_000011 on rc1a-dataproc-d-1jipfldazn1nw9u1.mdb.yandexcloud.net_33887\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Tue Jan 24 16:42:38 +0000 2023\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "End of LogType:stderr\n",
      "***********************************************************************\n",
      "\n",
      "Container: container_1674572529553_0009_01_000043 on rc1a-dataproc-d-1jipfldazn1nw9u1.mdb.yandexcloud.net_33887\n",
      "LogAggregationType: AGGREGATED\n",
      "===============================================================================================================\n",
      "LogType:stderr\n",
      "LogLastModifiedTime:Tue Jan 24 16:42:38 +0000 2023\n",
      "LogLength:208\n",
      "LogContents:\n",
      "Traceback (most recent call last):\n",
      "  File \"mistake.py\", line 34, in <module>\n",
      "    {\n",
      "  File \"mistake.py\", line 21, in mapper\n",
      "    strange_number = 1.0 / (len(word) - 1)\n"
     ]
    }
   ],
   "source": [
    "! yarn logs -applicationId application_1674572529553_0009 -log_files stderr | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идентификатор можно найти логах запуска, в интерфейсе или посмотреть список всех и найти там"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-24 16:43:30,487 INFO client.RMProxy: Connecting to ResourceManager at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:8032\n",
      "2023-01-24 16:43:30,702 INFO client.AHSProxy: Connecting to Application History server at rc1a-dataproc-m-ymnek55ntqs3neie.mdb.yandexcloud.net/10.128.0.25:10200\n",
      "Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0\n",
      "                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\n"
     ]
    }
   ],
   "source": [
    "! yarn application -list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы заранее прикончить задачу, можно также использовать yarn\n",
    "\n",
    "```bash\n",
    "yarn application -kill <application_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переносим результаты\n",
    "\n",
    "Все данные, с которыми мы работали только что, лежат на жестких дисках кластера и доступны только через hdfs\n",
    "\n",
    "Если мы готовы презентовать наш результат миру, нужно его переместить в s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2023-01-24 16:20 /user/tweets/top10-stop-words/_SUCCESS\r\n",
      "-rw-r--r--   1 ubuntu hadoop        109 2023-01-24 16:20 /user/tweets/top10-stop-words/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/tweets/top10-stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-24 16:46:10,285 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-01-24 16:46:10,369 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-01-24 16:46:10,369 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "cp: `s3a://lsml-kosmos/top10.txt': File exists\n",
      "2023-01-24 16:46:13,241 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2023-01-24 16:46:13,242 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2023-01-24 16:46:13,243 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cp /user/tweets/top10-stop-words/part-00000 s3a://lsml-kosmos/top10.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно проверять через интерфейс - файл оказался в бакете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop жжет бабло\n",
    "\n",
    "<img src=\"http://vostokovod.ru/assets/images/blog/2013/000333.png\">\n",
    "\n",
    "Полноценный кластер - весьма дорогое удовольствие, поэтому отлючайте его после использования. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
