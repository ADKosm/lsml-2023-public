{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запуск на устройствах пользователей\n",
    "\n",
    "Чтобы запустить стенд с TfJS , перейдите в директорию `tensorflowjs` и запустите\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "После этого можно переходить по адресам\n",
    "* http://localhost:9090\n",
    "* http://localhost:9091"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j81mzLx40eD"
   },
   "source": [
    "## PyTorch. DistributedDataParallel \n",
    "\n",
    "Это класс, скрывающий под капотом детали параллельного обучениия в PyTorch, применяется для обучения на кластере из нескольких компьютеров с несколькими GPU\n",
    "\n",
    "Внутри осуществляет разбивку данных по обработчикам, на backward шаге градиенты усредняются с использованием allreduce.\n",
    "\n",
    "Существует также `DataParallel` класс, но он работает в рамках одного процесса, используя потоки, тем самым он не рекомендуется к применению в силу ограничений GIL, `DistributedDataParallel` показывает себя лучше даже при обучении на одном компьютере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9i8Z9Ku4wGU",
    "outputId": "4b40cf10-d97e-4118-82ad-0fc6f5a5cacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting launch_ddp_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile launch_ddp_demo.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # инициализация группы процессов\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))\n",
    "\n",
    "\n",
    "def demo_basic(rank, world_size):\n",
    "    print(f\"Базовый пример использованиия DDP с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # создать модель и отправить её на GPU с id = rank\n",
    "    model = ToyModel()\n",
    "    ddp_model = DDP(model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "def run_demo(demo_fn, world_size):\n",
    "    mp.spawn(demo_fn,\n",
    "             args=(world_size,),\n",
    "             nprocs=world_size,\n",
    "             join=True)\n",
    "    \n",
    "def demo_checkpoint(rank, world_size):\n",
    "    print(f\"Пример использованиия DDP с чекпоинтами с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    model = ToyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n",
    "    if rank == 0:\n",
    "        # Все процессы должны начать с одних значений параметров и градиента,\n",
    "        # они также синхронизируются на backward шаге,\n",
    "        # поэтому достаточно сохранять модель в одном процессе\n",
    "        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n",
    "\n",
    "    # Используем barrier(), чтобы удостовериться,\n",
    "    # что процесс 1 загрузит модель после сохранения процессом 0\n",
    "    dist.barrier()\n",
    "    # опциии map_location\n",
    "    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n",
    "    ddp_model.load_state_dict(\n",
    "        torch.load(CHECKPOINT_PATH, map_location=map_location))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = ddp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(rank)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if rank == 0:\n",
    "        os.remove(CHECKPOINT_PATH)\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "class ToyMpModel(nn.Module):\n",
    "    def __init__(self, dev0, dev1):\n",
    "        super(ToyMpModel, self).__init__()\n",
    "        self.dev0 = dev0\n",
    "        self.dev1 = dev1\n",
    "        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(self.dev0)\n",
    "        x = self.relu(self.net1(x))\n",
    "        x = x.to(self.dev1)\n",
    "        return self.net2(x)\n",
    "\n",
    "def demo_model_parallel(rank, world_size):\n",
    "    print(f\"Пример использованиия DDP с параллельностью по модели с рангом {rank}.\")\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    # настроим модель и устройства в этом процессе\n",
    "    dev0 = rank * 2\n",
    "    dev1 = rank * 2 + 1\n",
    "    mp_model = ToyMpModel(dev0, dev1)\n",
    "    ddp_mp_model = DDP(mp_model)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # вывод будет на устройстве dev1\n",
    "    outputs = ddp_mp_model(torch.randn(20, 10))\n",
    "    labels = torch.randn(20, 5).to(dev1)\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(\"Количество GPU:\", n_gpus)\n",
    "    run_demo(demo_basic, 4)\n",
    "    run_demo(demo_checkpoint, n_gpus)\n",
    "    # run_demo(demo_model_parallel, n_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pTnDeykLA8wJ",
    "outputId": "cbc01f90-3501-48a3-f86e-a8fead68f44d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество GPU: 0\n",
      "Базовый пример использованиия DDP с рангом 0.\n",
      "Базовый пример использованиия DDP с рангом 1.\n",
      "Базовый пример использованиия DDP с рангом 3.\n",
      "Базовый пример использованиия DDP с рангом 2.\n"
     ]
    }
   ],
   "source": [
    "! python3 launch_ddp_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0YIfRC049CX"
   },
   "source": [
    "Для сохранениия и загрузки состояния обучения (checkpoint) используются функциии `torch.save()` и `torch.load()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJispqHd5C88"
   },
   "source": [
    "Model Parallel - использование нескольких устройств на одном обработчике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_-HisF2HzVg"
   },
   "source": [
    "### Обучение MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oj92iYqL9wal",
    "outputId": "87d1a028-a42e-4762-82bb-3610ec518c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting launch_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile launch_ddp.py\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--nodes', default=1, type=int, metavar='N',\n",
    "                        help='количество обработчиков (default: 1)')\n",
    "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
    "                        help='глобальный ранг')\n",
    "    parser.add_argument('--epochs', default=2, type=int, metavar='N',\n",
    "                        help='количество эпох обучениия')\n",
    "    args = parser.parse_args()\n",
    "    args.world_size = args.nodes\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '23334'\n",
    "    print(\"Run with args - {}\".format(str(args)))\n",
    "    mp.spawn(train, nprocs=args.nodes, args=(args,))\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train(rank, args):\n",
    "    dist.init_process_group(backend='gloo', init_method='env://', world_size=args.world_size, rank=rank)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    print(\"worker {}\".format(rank))\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    model = ConvNet()\n",
    "    torch.cuda.set_device(device)\n",
    "    model.cuda(device)\n",
    "    batch_size = 100\n",
    "    \n",
    "    # определим loss function и optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 1e-4)\n",
    "    # Обернем модель в DDP\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[device])\n",
    "    # Загрузка данных\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                               train=True,\n",
    "                                               transform=transforms.ToTensor(),\n",
    "                                               download=True)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n",
    "                                                                    num_replicas=args.world_size,\n",
    "                                                                    rank=rank)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               sampler=train_sampler)\n",
    "\n",
    "    start = datetime.now()\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            labels = labels.cuda(non_blocking=True)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('[{}] Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(rank, epoch + 1, args.epochs, i + 1, total_step,\n",
    "                                                                         loss.item()))\n",
    "    if rank == 0:\n",
    "        print(\"Обучение завершено за: \" + str(datetime.now() - start))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8gitSAujJC5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WfZ2cl590da",
    "outputId": "a3cf9219-e045-4c81-cbe0-aa1434400275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with args - Namespace(epochs=2, nodes=2, nr=0, world_size=2)\n",
      "worker 1\n",
      "worker 0\n",
      "Traceback (most recent call last):\n",
      "  File \"launch_ddp.py\", line 108, in <module>\n",
      "    main()\n",
      "  File \"launch_ddp.py\", line 27, in main\n",
      "    mp.spawn(train, nprocs=args.nodes, args=(args,))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 240, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 198, in start_processes\n",
      "    while not context.join():\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 160, in join\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "\n",
      "-- Process 0 terminated with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n",
      "    fn(i, *args)\n",
      "  File \"/home/ubuntu/launch_ddp.py\", line 62, in train\n",
      "    torch.cuda.set_device(device)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/cuda/__init__.py\", line 311, in set_device\n",
      "    device = _get_device_index(device)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/cuda/_utils.py\", line 30, in _get_device_index\n",
      "    raise ValueError('Expected a cuda device, but got: {}'.format(device))\n",
      "ValueError: Expected a cuda device, but got: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python3 launch_ddp.py --nodes 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nvs5y_2m-vyK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3j2XAdb88Ku"
   },
   "source": [
    "### Распределенное обучениеи с использованием PyTorch и Horovod для MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4B2UmzpUP51S",
    "outputId": "cf989d3a-a901-40ee-d9d3-3387c794a5de"
   },
   "outputs": [],
   "source": [
    "! pip install horovod[pytorch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdyzttzL88Kv",
    "outputId": "85721184-59b2-4c31-a1b3-692fe06a1768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting learn_hvd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile learn_hvd.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "# Параметры обучения\n",
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "def train_one_epoch(model, device, data_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader) * len(data),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "            \n",
    "from time import time\n",
    "import os\n",
    "\n",
    "LOG_DIR = os.path.join('./logs/', str(time()), 'MNISTDemo')\n",
    "os.makedirs(LOG_DIR)\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch):\n",
    "  filepath = LOG_DIR + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n",
    "  state = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "  }\n",
    "  torch.save(state, filepath)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def train(learning_rate):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  train_dataset = datasets.MNIST(\n",
    "    'data', \n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "  data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  model = Net().to(device)\n",
    "\n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    train_one_epoch(model, device, data_loader, optimizer, epoch)\n",
    "    save_checkpoint(model, optimizer, epoch)\n",
    "\n",
    "import horovod.torch as hvd\n",
    "\n",
    "def train_hvd(learning_rate):\n",
    "  hvd.init()  # Иницииализация\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  \n",
    "  if device.type == 'cuda':\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "  train_dataset = datasets.MNIST(\n",
    "    root='data-%d'% hvd.rank(),  # каждый обработчики в своей папке\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "  )\n",
    "\n",
    "  from torch.utils.data.distributed import DistributedSampler\n",
    "  \n",
    "  train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "  model = Net().to(device)\n",
    "  \n",
    "  optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum=momentum)\n",
    "\n",
    "  # оборачиваем оптимизатор в Horovod DistributedOptimizer\n",
    "  optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n",
    "  \n",
    "  # Ставим для всех моделей начальные параметры одинаковыми\n",
    "  hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n",
    "\n",
    "  for epoch in range(1, num_epochs + 1):\n",
    "    train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
    "    # Сохраняем только в одном обработчике\n",
    "    if hvd.rank() == 0:\n",
    "      save_checkpoint(model, optimizer, epoch)\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  train_hvd(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "srAHAFohPwA1"
   },
   "outputs": [],
   "source": [
    "from learn_hvd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxaKBq5v88Kw"
   },
   "source": [
    "### Обучение MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "YM7O3vaz88Kw",
    "outputId": "e4bcdb41-471c-4f61-f7b2-8490b6620cd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/learn_hvd.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.327062\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.314169\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.300497\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.284976\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.269949\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.262890\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.243164\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.217157\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.170468\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.160197\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.101226\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.046172\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.969879\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.854494\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.720318\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.464677\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.502841\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.449594\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.318620\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.174082\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.356215\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.211502\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.091241\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.997741\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.937607\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.984210\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.881794\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.880659\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.061545\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.855154\n"
     ]
    }
   ],
   "source": [
    "train(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP5hSCOE88Kw"
   },
   "source": [
    "### Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8H8zz3VXTBox",
    "outputId": "4150667c-75ca-4888-df48-9bb964631f48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.326104\n",
      "Train Epoch: 1 [0/30000 (0%)]\tLoss: 2.347001\n",
      "Train Epoch: 1 [10000/30000 (33%)]\tLoss: 2.288108\n",
      "Train Epoch: 1 [10000/30000 (33%)]\tLoss: 2.280800\n",
      "Train Epoch: 1 [20000/30000 (67%)]\tLoss: 2.257359\n",
      "Train Epoch: 1 [20000/30000 (67%)]\tLoss: 2.272566\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 2.265738\n",
      "Train Epoch: 2 [0/30000 (0%)]\tLoss: 2.211262\n",
      "Train Epoch: 2 [10000/30000 (33%)]\tLoss: 2.142014\n",
      "Train Epoch: 2 [10000/30000 (33%)]\tLoss: 2.156749\n",
      "Train Epoch: 2 [20000/30000 (67%)]\tLoss: 2.049343\n",
      "Train Epoch: 2 [20000/30000 (67%)]\tLoss: 2.024050\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 1.859179\n",
      "Train Epoch: 3 [0/30000 (0%)]\tLoss: 1.907485\n",
      "Train Epoch: 3 [10000/30000 (33%)]\tLoss: 1.710988\n",
      "Train Epoch: 3 [10000/30000 (33%)]\tLoss: 1.564717\n",
      "Train Epoch: 3 [20000/30000 (67%)]\tLoss: 1.435290\n",
      "Train Epoch: 3 [20000/30000 (67%)]\tLoss: 1.591935\n",
      "Train Epoch: 4 [0/30000 (0%)]\tLoss: 1.251025\n",
      "Train Epoch: 4 [0/30000 (0%)]\tLoss: 1.477257\n",
      "Train Epoch: 4 [10000/30000 (33%)]\tLoss: 1.075023\n",
      "Train Epoch: 4 [10000/30000 (33%)]\tLoss: 1.309752\n",
      "Train Epoch: 4 [20000/30000 (67%)]\tLoss: 1.146683\n",
      "Train Epoch: 4 [20000/30000 (67%)]\tLoss: 1.088512\n",
      "Train Epoch: 5 [0/30000 (0%)]\tLoss: 1.075019\n",
      "Train Epoch: 5 [0/30000 (0%)]\tLoss: 1.142342\n",
      "Train Epoch: 5 [10000/30000 (33%)]\tLoss: 0.914001\n",
      "Train Epoch: 5 [10000/30000 (33%)]\tLoss: 0.972719\n",
      "Train Epoch: 5 [20000/30000 (67%)]\tLoss: 0.967985\n",
      "Train Epoch: 5 [20000/30000 (67%)]\tLoss: 0.777582\n",
      "learn_hvd.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "learn_hvd.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "! mpirun -np 2 -H localhost:2 python3 learn_hvd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "final09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
